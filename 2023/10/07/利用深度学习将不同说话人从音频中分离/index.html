<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>利用深度学习将不同说话人从音频中分离 | 一世逍遥的博客</title><meta name="author" content="LiSheng"><meta name="copyright" content="LiSheng"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="ffffff"><meta name="description" content="今天我分享一种通过深度学习将不同说话人从音频文件中分离的方法，这个方法的契机是我想从《哆啦A梦》的动画片里面分离主角们的说的台词。我个人是音频处理的门外汉，对很多音频处理的事情知之甚少，很多都是通过直接调用别人训练好的模型或者网上找教程现学现用的，所以如果有错误或者更好的方法，恳请读者批评指正。那接下来进入正题。 方法概述 从音频里面分离不同的说话人是音频领域的研究热点，这个任务的英文名称是说话人">
<meta property="og:type" content="article">
<meta property="og:title" content="利用深度学习将不同说话人从音频中分离">
<meta property="og:url" content="https://lisheng2001.github.io/2023/10/07/%E5%88%A9%E7%94%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B0%86%E4%B8%8D%E5%90%8C%E8%AF%B4%E8%AF%9D%E4%BA%BA%E4%BB%8E%E9%9F%B3%E9%A2%91%E4%B8%AD%E5%88%86%E7%A6%BB/index.html">
<meta property="og:site_name" content="一世逍遥的博客">
<meta property="og:description" content="今天我分享一种通过深度学习将不同说话人从音频文件中分离的方法，这个方法的契机是我想从《哆啦A梦》的动画片里面分离主角们的说的台词。我个人是音频处理的门外汉，对很多音频处理的事情知之甚少，很多都是通过直接调用别人训练好的模型或者网上找教程现学现用的，所以如果有错误或者更好的方法，恳请读者批评指正。那接下来进入正题。 方法概述 从音频里面分离不同的说话人是音频领域的研究热点，这个任务的英文名称是说话人">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://lisheng2001.github.io/images/avatar.webp">
<meta property="article:published_time" content="2023-10-07T10:17:39.000Z">
<meta property="article:modified_time" content="2025-03-28T15:16:42.461Z">
<meta property="article:author" content="LiSheng">
<meta property="article:tag" content="多模态">
<meta property="article:tag" content="深度学习">
<meta property="article:tag" content="语音识别">
<meta property="article:tag" content="说话人分类">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://lisheng2001.github.io/images/avatar.webp"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://lisheng2001.github.io/2023/10/07/%E5%88%A9%E7%94%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B0%86%E4%B8%8D%E5%90%8C%E8%AF%B4%E8%AF%9D%E4%BA%BA%E4%BB%8E%E9%9F%B3%E9%A2%91%E4%B8%AD%E5%88%86%E7%A6%BB/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '利用深度学习将不同说话人从音频中分离',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2025-03-28 23:16:42'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', 'ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/images/avatar.webp" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">51</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">71</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">12</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="not-top-img" id="page-header"><nav id="nav"><span id="blog-info"><a href="/" title="一世逍遥的博客"><span class="site-name">一世逍遥的博客</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav></header><main class="layout" id="content-inner"><div id="post"><div id="post-info"><h1 class="post-title">利用深度学习将不同说话人从音频中分离</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-10-07T10:17:39.000Z" title="发表于 2023-10-07 18:17:39">2023-10-07</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-03-28T15:16:42.461Z" title="更新于 2025-03-28 23:16:42">2025-03-28</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="利用深度学习将不同说话人从音频中分离"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div><article class="post-content" id="article-container"><p>今天我分享一种通过深度学习将不同说话人从音频文件中分离的方法，这个方法的契机是我想从《哆啦A梦》的动画片里面分离主角们的说的台词。我个人是音频处理的门外汉，对很多音频处理的事情知之甚少，很多都是通过直接调用别人训练好的模型或者网上找教程现学现用的，所以如果有错误或者更好的方法，恳请读者批评指正。那接下来进入正题。</p>
<h1 id="方法概述">方法概述</h1>
<p>从音频里面分离不同的说话人是音频领域的研究热点，这个任务的英文名称是说话人分类(Speaker
Diarization)，已经有很多研究者提出了各种解决方案。但如果按切分依据主要可以分为两种：第一种是单纯依赖音频信息的单模态说话人识别；第二种是结合语音识别(Auto
Speaker Recognition, ASR)技术的多模态说话人识别。</p>
<p>在有较多可用数据的情况下，单模态说话人识别就能达到比较好的效果了，比如开源库<a
target="_blank" rel="noopener" href="https://github.com/pyannote/pyannote-audio"><code>pyannote</code></a>就提供了在英文数据集上预训练过的模型，在很多英文场景下使用作者提供的<code>speaker-diarazation</code>管道应该就能达到不错的说话人分类效果。对于<code>pyannote</code>是通过这样的流程完成整个说话人分类的：</p>
<figure>
<img src="/images/image-20231007185745252.png"
alt="pyannote进行说话人分类" />
<figcaption aria-hidden="true">pyannote进行说话人分类</figcaption>
</figure>
<p>可以看到它通过说话活动检测(Voice Activity Detection,
VAD)先将音频种有人说话的部分提取出来，之后再通过说话人转换点检测(Speaker
Change Detection,
SCD)进一步预测不同说话人切换的位置，从而对不同说话人在一块的音频进行分离，这样能够得到只有单个说话人的说话片段（实际这个库还考虑了多个说话人同时说话的情况，因为考虑这个问题就比较复杂了，所以略去这一部分）。在得到只有单人说话人片段之后再利用模型提取该片段的说话人嵌入(Speaker
Embedding)，之后通过聚类算法将音色相同的说话人聚合在一起，从而完成说话人分类。</p>
<p>但这个方法需要带说话人转换点的注释的多说话人的数据集或者在对应语言上预训练过的说话人转换点检测模型，不幸地是，目前<code>pyannote</code>并没有提供中文的说话人检测模型，这可能是在我的数据集上<code>pyannote</code>并没有表现出良好的分割性能的原因之一。</p>
<p>如果我们想提升在自己数据集上的分割性能，作者也提供了去微调这些模型以适应新数据集的方法，但人工标记说话人转换点非常地消耗时间，而且需要一定的数据量才能生效，我并没有相应的时间和金钱这样精细地标注我的数据集。</p>
<p>于是我们将目光放在了第二种方法上，即结合ASR的说话人分类。自从Whisper这样性能优越的语音识别模型被提出，结合ASR进行各种音频任务也成为了研究的热点，arXiv上有不少结合Whisper等ASR模型进行说话人分类。这类方法的优点是可以结合转录出来的语义信息，比如转录出","或者"."可以标识一段话的暂停或结尾。更进一步地，对中文来说，一些符号如”，“、”。“、”！“、”……“、”？“等能够标识一些短句，如果结合预测字级时间戳模型就可以获取每个短句的切分了。幸运地是，相比于说话人分类，语音识别ASR是一个更加基础也更加成熟的领域。国内一些大厂如阿里、百度等都开发了适配中文的语音识别模型并且能达到不错的准确率。因此笔者认为在中文环境下，通过ASR结合标点符号将音频切割为短句，并假定每个短句里只有一个说话人是一种低成本而且效果不错的方法。因此这篇文章将使用ASR进行分割，然后再进行说话人识别。</p>
<p>在正式进入方法之前，我需要说明这个方法的优点与缺点，以确认这个方法是否真的是你需要的。</p>
<p>优点：</p>
<ul>
<li>不需要训练或者微调分割模型，而是通过ASR转录出的文本决定切分位置。</li>
<li>可以渐进式地训练说话人分类模型，如果你有很多类似于动画片一集一集的音频数据，你可以先注释一个基础数据集子集，然后训练一个基础的说话人模型，之后用对另外的集进行分类，之后你手动纠正错误分类的片段到正确的位置从而构成一个新的数据集，将这个数据集合入之前的训练集后我们可以训练一个性能更好的说话人分类模型，可以重复这个过程直到你满意分类器的性能为止。</li>
</ul>
<p>缺点：</p>
<ul>
<li>通过文本模态进行分割依赖于标点注释模型的准确度和字级时间戳预测模型的准确度，因此它不一定能产生正确的分割。</li>
<li>本文的方法不是无监督的方法，对于说话人分类本文使用监督数据训练分类器而不是聚类，因此你需要注释一定量的数据。即使注释了数据后，训练的分类器也会产生分类误差，如果对说话人分类的精度要求很高的话请使用人工标注。</li>
<li>对于有很多人一起说话，即重叠语音的情况无能为力。本文的方法不能处理这些重叠语音的部分。</li>
</ul>
<p>如果你对它的优点感到满意而且能够容忍它的缺点，那我们正式进入方法具体流程部分。</p>
<h1 id="具体流程及供参考的实现代码">具体流程及供参考的实现代码</h1>
<h2 id="从视频中分离音频可选">从视频中分离音频（可选）</h2>
<p>我要将音频从视频里面分割出来所以有这个步骤，如果你已经有音频数据了就可以跳过这步。实现的方式主要是通过MoviePy这个库获取视频中的音频文件，裁剪片头片尾后保存为音频文件。这适用于片头和片尾一定在开始和结束部分，而且时长是固定的情况。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># pre_clip.py</span></span><br><span class="line"><span class="comment"># 预先裁剪视频来跳过片头片尾，防止干扰</span></span><br><span class="line"><span class="keyword">from</span> moviepy.editor <span class="keyword">import</span> VideoFileClip</span><br><span class="line"><span class="keyword">import</span> glob</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 配置mp4路径</span></span><br><span class="line">mp4_path = <span class="string">&quot;mp4路径&quot;</span></span><br><span class="line"><span class="comment"># 导出的音频文件路径</span></span><br><span class="line">audio_path = <span class="string">&quot;输出源文件音频路径&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 利用glob获取所有mp4文件</span></span><br><span class="line">mp4_list = glob.glob(os.path.join(mp4_path, <span class="string">&quot;*.mp4&quot;</span>))</span><br><span class="line">num = <span class="built_in">len</span>(mp4_list)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i, mp4 <span class="keyword">in</span> <span class="built_in">enumerate</span>(mp4_list):</span><br><span class="line">    <span class="comment"># 生成音频文件名</span></span><br><span class="line">    basename = os.path.basename(mp4)</span><br><span class="line">    basename, ext_name = os.path.splitext(basename)</span><br><span class="line">    basename = basename.replace(<span class="string">&quot; &quot;</span>, <span class="string">&quot;_&quot;</span>)   <span class="comment"># 将空格替换为_号</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> VideoFileClip(mp4) <span class="keyword">as</span> clip:</span><br><span class="line">        audio = clip.audio</span><br><span class="line">        <span class="comment"># 剪切片头和片尾</span></span><br><span class="line">        <span class="comment"># 对于我的情况，截掉前1分12秒和最后6秒</span></span><br><span class="line">        processed_audio = audio.subclip(<span class="number">72</span>, -<span class="number">6</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 保存片段到新文件夹中</span></span><br><span class="line">        processed_audio.write_audiofile(<span class="string">f&quot;<span class="subst">&#123;os.path.join(audio_path, basename)&#125;</span>.wav&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 指示进度</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;已处理: <span class="subst">&#123;i&#125;</span> / <span class="subst">&#123;num&#125;</span>  \r&quot;</span>, end=<span class="string">&quot;&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>这样应该在<code>audio_path</code>对应的目录里有所有视频对应的音频文件了。</p>
<h2 id="将人声与背景音乐分离可选">将人声与背景音乐分离（可选）</h2>
<p>这步我们会使用深度学习模型，没错就是<a
target="_blank" rel="noopener" href="https://github.com/Anjok07/ultimatevocalremovergui"><code>UVR5</code></a>这个项目训练的声乐分离模型来把人声从背景音乐中分离出来。我是在Linux机器上运行的，我会更喜欢命令行或者Python库的版本，这样更便于自动化，因此我选择了<a
target="_blank" rel="noopener" href="https://github.com/karaokenerds/python-audio-separator"><code>audio-separator</code></a>这个项目来运行UVR5训练的声乐分离模型。这一步同样是可选的，如果你认为你的音频数据集中并没有背景音乐的话这步也可以跳过。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># vocal_remove.py</span></span><br><span class="line"><span class="comment"># 分离人声</span></span><br><span class="line"><span class="keyword">from</span> audio_separator <span class="keyword">import</span> Separator</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> glob</span><br><span class="line"><span class="keyword">import</span> logging</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载所有音频文件</span></span><br><span class="line">audio_files = glob.glob(os.path.join(<span class="string">&quot;你的音频路径&quot;</span>, <span class="string">&quot;*.wav&quot;</span>))</span><br><span class="line">output_dir = <span class="string">&quot;分离人声处理后的音频的保存文件夹&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化分割器</span></span><br><span class="line"><span class="keyword">for</span> i, audio_file <span class="keyword">in</span> <span class="built_in">enumerate</span>(audio_files):</span><br><span class="line">    separator = Separator(audio_file, model_name=<span class="string">&#x27;UVR-MDX-NET-Voc_FT&#x27;</span>, output_single_stem=<span class="string">&quot;vocals&quot;</span>, </span><br><span class="line">                          use_cuda=<span class="literal">True</span>, output_dir=output_dir, log_level=logging.INFO)</span><br><span class="line">    separator.separate()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;已完成: <span class="subst">&#123;i&#125;</span>, 文件: <span class="subst">&#123;audio_file&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>这样应该能在<code>output_dir</code>里面找到只保留人声的音频了，这个分离人声的方法应该是还有提升空间的，如果你追求更好的分离人声表现，可以换用更好的人声分离方法。</p>
<h2 id="asr预切分">ASR预切分</h2>
<p>当音频文件就位之后，我们开始预切分一些视频来供说话人识别打标。这里使用阿里的<a
target="_blank" rel="noopener" href="https://github.com/alibaba-damo-academy/FunASR"><code>FUNASR</code></a>项目来处理音频文件的语音识别，后续的说话人识别模型里的嵌入部分也将使用该项目的预训练嵌入模型。预切分就是不管说话人，直接按短句（也就是碰到”，“、”。“、”！“、”？“等都会被切分）分割整个音频。可以随机选择一些文件进行切分。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ASR_pre_clip.py</span></span><br><span class="line"><span class="comment"># 通过ASR进行预切分</span></span><br><span class="line"><span class="keyword">from</span> modelscope.pipelines <span class="keyword">import</span> pipeline</span><br><span class="line"><span class="keyword">from</span> modelscope.utils.constant <span class="keyword">import</span> Tasks</span><br><span class="line"><span class="keyword">from</span> moviepy.editor <span class="keyword">import</span> AudioFileClip</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="comment"># 最大句子间隔</span></span><br><span class="line">max_sentence_interval = <span class="number">800</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 最小识别块，默认只识别不小于1000ms的块</span></span><br><span class="line">min_chunk_length = <span class="number">1000</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 需要预切分的文件路径</span></span><br><span class="line">audio_file = <span class="string">&quot;需要预切分的文件路径&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出路径</span></span><br><span class="line">chunks_dir = <span class="string">&quot;切分结果输出文件夹路径&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">inference_pipeline = pipeline(</span><br><span class="line">    task=Tasks.auto_speech_recognition,</span><br><span class="line">    model=<span class="string">&#x27;damo/speech_paraformer-large-vad-punc_asr_nat-zh-cn-16k-common-vocab8404-pytorch&#x27;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 转录音频</span></span><br><span class="line">rec_result = inference_pipeline(audio_in=audio_file)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将太远的词分割开，单独作为一个句子推理</span></span><br><span class="line">new_sentences = []</span><br><span class="line"><span class="keyword">for</span> sentence <span class="keyword">in</span> rec_result[<span class="string">&quot;sentences&quot;</span>]:</span><br><span class="line">    <span class="comment"># 准备左右对齐</span></span><br><span class="line">    ts_list = sentence[<span class="string">&quot;ts_list&quot;</span>]</span><br><span class="line">    word_seg_list = sentence[<span class="string">&quot;text_seg&quot;</span>].split(<span class="string">&quot; &quot;</span>)[:-<span class="number">1</span>]</span><br><span class="line">    text = sentence[<span class="string">&quot;text&quot;</span>]</span><br><span class="line">    start = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(ts_list)):</span><br><span class="line">        <span class="comment"># 检查前一个词的结束时间和后一个词的开始时间是否超出阈值</span></span><br><span class="line">        <span class="comment"># 如果超过阈值就手动把它们分隔开</span></span><br><span class="line">        <span class="keyword">if</span> ts_list[i][<span class="number">0</span>] - ts_list[i-<span class="number">1</span>][<span class="number">1</span>] &gt; max_sentence_interval:</span><br><span class="line">            <span class="comment"># 寻找第i个值并把start到i-1的字符加入到新文本中</span></span><br><span class="line">            <span class="comment"># 注意忽略掉新句子中的符号</span></span><br><span class="line">            <span class="comment"># 其实我感觉有点多余，因为这边的转录结果是不用的，但是还是这么做一下吧</span></span><br><span class="line">            rest_of_char = i - start</span><br><span class="line">            new_text = <span class="string">&quot;&quot;</span></span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(start, <span class="built_in">len</span>(text)):</span><br><span class="line">                <span class="keyword">if</span> rest_of_char &gt; <span class="number">0</span>:</span><br><span class="line">                    new_text += text[j]</span><br><span class="line"></span><br><span class="line">                    <span class="keyword">if</span> text[j] <span class="keyword">not</span> <span class="keyword">in</span> [<span class="string">&quot;，&quot;</span>, <span class="string">&quot;。&quot;</span>, <span class="string">&quot;！&quot;</span>]:</span><br><span class="line">                        rest_of_char -= <span class="number">1</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">            <span class="comment"># 合成新句子</span></span><br><span class="line">            new_sentence_data = &#123;</span><br><span class="line">                <span class="string">&quot;text&quot;</span>: new_text + <span class="string">&quot;。&quot;</span>, </span><br><span class="line">                <span class="string">&quot;start&quot;</span>: ts_list[start][<span class="number">0</span>], </span><br><span class="line">                <span class="string">&quot;end&quot;</span>: ts_list[i-<span class="number">1</span>][<span class="number">1</span>], </span><br><span class="line">                <span class="string">&quot;ts_list&quot;</span>: [ts_list[k] <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(start, i)]</span><br><span class="line">            &#125;</span><br><span class="line">            new_sentences.append(new_sentence_data)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 处理善后</span></span><br><span class="line">            <span class="comment"># print(j)</span></span><br><span class="line">            start = j</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 如果剩下的都正常就把关键信息提取并加进去</span></span><br><span class="line">    new_sentence_data = &#123;</span><br><span class="line">        <span class="string">&quot;text&quot;</span>: text[start: ], </span><br><span class="line">        <span class="string">&quot;start&quot;</span>: ts_list[start][<span class="number">0</span>], </span><br><span class="line">        <span class="string">&quot;end&quot;</span>: ts_list[-<span class="number">1</span>][<span class="number">1</span>], </span><br><span class="line">        <span class="string">&quot;ts_list&quot;</span>: [ts_list[k] <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(start, <span class="built_in">len</span>(ts_list))]</span><br><span class="line">    &#125;</span><br><span class="line">    new_sentences.append(new_sentence_data)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 切分音频并保存</span></span><br><span class="line">basename = os.path.basename(audio_file)</span><br><span class="line">basename, ext_name = os.path.splitext(basename)</span><br><span class="line"><span class="comment"># 清除掉多余的后缀，如果需要的话...</span></span><br><span class="line">basename = basename.replace(<span class="string">&quot;_(Vocals)_UVR-MDX-NET-Voc_FT&quot;</span>, <span class="string">&quot;&quot;</span>)</span><br><span class="line"><span class="comment"># 保存音频的代码还是用AudioFileClip吧，用别的有点不习惯</span></span><br><span class="line">audio = AudioFileClip(audio_file)</span><br><span class="line"></span><br><span class="line">j = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> sentence <span class="keyword">in</span> new_sentences:</span><br><span class="line">    <span class="comment"># 跳过太小的块，这部分识别也比较难识别准，还会多出很多碎片</span></span><br><span class="line">    <span class="keyword">if</span> sentence[<span class="string">&quot;end&quot;</span>] - sentence[<span class="string">&quot;start&quot;</span>] &lt; min_chunk_length:</span><br><span class="line">        <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 注意毫秒与秒的转换</span></span><br><span class="line">    start = sentence[<span class="string">&quot;start&quot;</span>] / <span class="number">1000</span></span><br><span class="line">    end = sentence[<span class="string">&quot;end&quot;</span>] / <span class="number">1000</span></span><br><span class="line"></span><br><span class="line">    j += <span class="number">1</span></span><br><span class="line">    audio_chunk = audio.subclip(start, end)</span><br><span class="line">    <span class="comment"># 创建文件夹</span></span><br><span class="line">    os.makedirs(os.path.join(chunks_dir), exist_ok=<span class="literal">True</span>)</span><br><span class="line">    audio_chunk.write_audiofile(<span class="string">f&quot;<span class="subst">&#123;os.path.join(chunks_dir, basename)&#125;</span>_<span class="subst">&#123;j&#125;</span><span class="subst">&#123;ext_name&#125;</span>&quot;</span>)</span><br><span class="line">    </span><br><span class="line">audio.close()</span><br></pre></td></tr></table></figure>
<p>我建议如果一个音频就能包含所有你关注的说话人角色的话，那就只切分一个。原则就是在保证所有说话人至少能找到一条样本数据的情况下尽量少切，毕竟这里纯体力活动很难受。如果一条音频不足以覆盖所有你感兴趣的说话人，那就更换几个音频重新运行脚本直到满足原则。这样在<code>chunks_dir</code>下就会有很多没有标注过，但是都是短句的音频了。</p>
<h2 id="说话人分类基础模型训练">说话人分类基础模型训练</h2>
<p>在训练之前我们先说一下这个所谓的说话人分类模型能做什么，它的作用是对它输入一段音频，它会将这段音频映射到一个说话人上，这也就完成了说话人分类。之后说一下这个分类器的结构，我们在这里要训练的说话人模型主要由两部分组成，一部分是在语音数据上预训练的说话人验证模型用于对音频提取嵌入(Embedding)，这部分不会参与训练，它只负责特征提取；另一部分是在嵌入(Embedding)之上构建的LDA模型，我使用的是<code>scikit learn</code>的LDA实现，该模型相关的文档可以在<a
target="_blank" rel="noopener" href="https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html">这个地方</a>找到。</p>
<p>语音嵌入使用阿里<code>FUNASR</code>预训练的<code>damo/speech_eres2net_large_sv_zh-cn_3dspeaker_16k</code>模型提取嵌入，然后将嵌入输入LDA完成分类。关于说话人设置上，如果你确信音频库里只有简单的几个人，那么你可以将说话人设置为那几个人。另一种情况是除了你感兴趣的几个主角团外每集还可能出现一些额外NPC，比如《哆啦A梦》里面出木衫可能会客串几集。我建议是新增两个额外的说话人用来标识男声和女声NPC，比如我可能会命名为<code>qitanan</code>（其他·男声）和<code>qitanv</code>（其他·女声）。</p>
<p>了解了这些之后，应该就可以对之前ASR预切分的数据进行打标了！你要做的就是用耳朵听这些短句的语音，然后辨别它们是哪些说话人。一个小tips就是合理使用vscode右键的”复制路径“功能可以事半功倍。好的，现在直接来看训练脚本：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># feature_voice.py</span></span><br><span class="line"><span class="comment"># 通过收集的声纹音频和damo/speech_eres2net_large_sv_zh-cn_3dspeaker_16k说话人识别模型的嵌入</span></span><br><span class="line"><span class="comment"># 借助FLDA，训练一个说话人识别的模型</span></span><br><span class="line"><span class="keyword">from</span> modelscope.pipelines <span class="keyword">import</span> pipeline</span><br><span class="line"><span class="keyword">from</span> modelscope.utils.constant <span class="keyword">import</span> Tasks</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.discriminant_analysis <span class="keyword">import</span> LinearDiscriminantAnalysis</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> glob</span><br><span class="line"><span class="keyword">import</span> shutil</span><br><span class="line"><span class="keyword">import</span> joblib</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> librosa</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 额外监督数据，如果生成的片段进行了校准，可以加入数据集中训练更好的标注模型</span></span><br><span class="line">extra_data_dirs = []</span><br><span class="line"></span><br><span class="line">sv_pipline = pipeline(</span><br><span class="line">    task=<span class="string">&#x27;speaker-verification&#x27;</span>,</span><br><span class="line">    model=<span class="string">&#x27;damo/speech_eres2net_large_sv_zh-cn_3dspeaker_16k&#x27;</span>,</span><br><span class="line">    model_revision=<span class="string">&#x27;v1.0.0&#x27;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过data训练一个FLDA判别器用于判别声音</span></span><br><span class="line">data = &#123;</span><br><span class="line">    <span class="string">&quot;daxiong&quot;</span>: [</span><br><span class="line">        <span class="string">&quot;daxiong的语音路径，有多个就在下面写多个，多多益善&quot;</span>, </span><br><span class="line">        <span class="string">&quot;daxiong的第二条语音路径&quot;</span>, </span><br><span class="line">    ], </span><br><span class="line">    , </span><br><span class="line">    <span class="string">&quot;duola&quot;</span>: [</span><br><span class="line">        <span class="string">&quot;duola的语音路径&quot;</span>, </span><br><span class="line">    ], </span><br><span class="line">    <span class="string">&quot;panghu&quot;</span>: [</span><br><span class="line">        <span class="string">&quot;panghu的语音路径&quot;</span>, </span><br><span class="line">    ], </span><br><span class="line">    <span class="string">&quot;xiaofu&quot;</span>: [</span><br><span class="line">        <span class="string">&quot;xiaofu的&quot;</span>, </span><br><span class="line">    ], </span><br><span class="line">    <span class="string">&quot;qitanan&quot;</span>: [</span><br><span class="line">        <span class="string">&quot;其他·男声的语音路径&quot;</span>, </span><br><span class="line">    ]</span><br><span class="line">    <span class="string">&quot;qitanv&quot;</span>: [</span><br><span class="line">        <span class="string">&quot;其他·女声的语音路径&quot;</span>, </span><br><span class="line">    ], </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 备份特征声音到指定文件夹，防止删除后丢失，也方便检查</span></span><br><span class="line">backups_dir = <span class="string">&quot;备份文件夹&quot;</span></span><br><span class="line"><span class="keyword">for</span> speaker <span class="keyword">in</span> data.keys():</span><br><span class="line">    os.makedirs(os.path.join(backups_dir, speaker), exist_ok=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">for</span> i, audio_file <span class="keyword">in</span> <span class="built_in">enumerate</span>(data[speaker]):</span><br><span class="line">        <span class="comment"># 备份文件</span></span><br><span class="line">        basename = os.path.basename(audio_file)</span><br><span class="line">        shutil.copy(audio_file, os.path.join(backups_dir, speaker, basename))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 后面清洗出来的也可以通过文件夹的形式加入进去，获得更高准确度</span></span><br><span class="line"><span class="keyword">for</span> extra_data_dir <span class="keyword">in</span> extra_data_dirs:</span><br><span class="line">    speakers = os.listdir(extra_data_dir)</span><br><span class="line">    <span class="keyword">for</span> speaker <span class="keyword">in</span> speakers:</span><br><span class="line">        <span class="keyword">if</span> speaker <span class="keyword">not</span> <span class="keyword">in</span> data:</span><br><span class="line">            <span class="keyword">raise</span> Exception(<span class="string">f&quot;不存在的角色: <span class="subst">&#123;speaker&#125;</span>，请检查额外数据集: <span class="subst">&#123;extra_data_dir&#125;</span>是否有不存在的角色。&quot;</span>)</span><br><span class="line">        </span><br><span class="line">        speaker_wavs_path = glob.glob(os.path.join(extra_data_dir, speaker, <span class="string">&quot;*.wav&quot;</span>))</span><br><span class="line">        <span class="keyword">for</span> speaker_wav_path <span class="keyword">in</span> speaker_wavs_path:</span><br><span class="line">            data[speaker].append(speaker_wav_path)</span><br><span class="line">        </span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取嵌入</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_embedding_by_file</span>(<span class="params">audio_path: <span class="built_in">str</span></span>):</span><br><span class="line">    <span class="comment"># 把模型移到cuda上</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">next</span>(sv_pipline.model.parameters()).device.<span class="built_in">type</span> != <span class="string">&quot;cuda&quot;</span>:</span><br><span class="line">        sv_pipline.model.to(<span class="string">&quot;cuda&quot;</span>)</span><br><span class="line"></span><br><span class="line">    data, fs = librosa.load(audio_path, sr=sv_pipline.model_config[<span class="string">&#x27;sample_rate&#x27;</span>])</span><br><span class="line">    output = torch.from_numpy(data).unsqueeze(<span class="number">0</span>)</span><br><span class="line">    output = output.to(<span class="string">&quot;cuda&quot;</span>)</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        embedding = sv_pipline.model(output)</span><br><span class="line">    <span class="keyword">return</span> embedding</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 构造数据集</span></span><br><span class="line">X = []</span><br><span class="line">Y = []</span><br><span class="line">id2speaker = &#123;&#125;</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;构造数据集中...&quot;</span>)</span><br><span class="line"></span><br><span class="line">speakers_num = <span class="built_in">len</span>(data)</span><br><span class="line"><span class="keyword">for</span> i, speaker <span class="keyword">in</span> <span class="built_in">enumerate</span>(data.keys()):</span><br><span class="line">    id2speaker[i] = speaker</span><br><span class="line">    <span class="comment"># 对于每个音频逐个获取嵌入</span></span><br><span class="line">    <span class="keyword">for</span> audio_file <span class="keyword">in</span> data[speaker]:</span><br><span class="line">        e = get_embedding_by_file(audio_file)</span><br><span class="line">        X.append(e.cpu().numpy())</span><br><span class="line">        Y.append(i)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;说话人ID: <span class="subst">&#123;i&#125;</span>, 说话人: <span class="subst">&#123;speaker&#125;</span> 的特征编码完毕 <span class="subst">&#123;i+<span class="number">1</span>&#125;</span> / <span class="subst">&#123;speakers_num&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;数据集构建完毕，现有标注数据条数: <span class="subst">&#123;<span class="built_in">len</span>(X)&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 切换为numpy数组</span></span><br><span class="line">X = np.array(X)</span><br><span class="line">X = X.reshape(X.shape[<span class="number">0</span>], -<span class="number">1</span>)</span><br><span class="line">Y = np.array(Y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练FLDA模型拟合</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;训练LDA模型中...&quot;</span>)</span><br><span class="line">clf = LinearDiscriminantAnalysis()</span><br><span class="line">clf.fit(X, Y)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;训练成功!准备保存模型...&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 保存训练好的模型</span></span><br><span class="line">model_path = <span class="string">&quot;模型保存文件夹/clf.joblib&quot;</span></span><br><span class="line">id2speaker_path = <span class="string">&quot;模型保存文件夹/id2speaker.json&quot;</span></span><br><span class="line">joblib.dump(clf, model_path)</span><br><span class="line">json.dump(id2speaker, <span class="built_in">open</span>(id2speaker_path, <span class="string">&quot;w&quot;</span>), ensure_ascii=<span class="literal">False</span>, indent=<span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;保存成功!&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>每次你听到某段声音是对应某个角色的，就右键复制该音频路径，并在<code>data</code>字典里对应角色的列表中添加该音频路径。如此往复，直到所有人的音频列表里都有至少一个（当然样本多的话是好事，后面分得准的话人工的操作就少，但至少有一个样本是原则）。现在你运行这个脚本它会训练一个说话人分类的基础模型并保存。</p>
<p>撒花，如果你已经成功训练了一个基础的说话人分类模型，那已经取得了一些的阶段性胜利啦。但是现在训练出来的说话人基础分类模型的分类能力还是比较弱的，我们需要再给它增添一些新样本学习更稳定、更有效的分类特征。</p>
<h2
id="渐进地提高说话人分类模型的性能循环多次">渐进地提高说话人分类模型的性能（循环多次）</h2>
<p>前面说到基础模型说话人分类的性能还比较薄弱，我们需要收集更多的样本来训练更好的分类器。幸运地是，现在分类器已经有一定能力将音频正确分类了，因此可以让这个基础模型先尝试去对切分的音频片段分类，然后我们来验证它分类的结果是否正确。如果我们对这个模型分类的结果进行验证，并把它分错的音频放置到正确的说话人角色下，我们就通过模型和人类配合的方式打标好了一个新的干净的数据集！这个干净的数据集又可以加入到原来的训练集里用于训练新的说话人分类模型，如此循环往复，随着标注数据越来越多，我们最终可以得到一个准确度令人满意的说话人分类模型。</p>
<p>现在我们把之前训练的说话人分类模型集成到ASR切分中：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># enhance_clf.py</span></span><br><span class="line"><span class="comment"># 渐进式增强说话人分类模型性能</span></span><br><span class="line"><span class="keyword">from</span> modelscope.pipelines <span class="keyword">import</span> pipeline</span><br><span class="line"><span class="keyword">from</span> modelscope.utils.constant <span class="keyword">import</span> Tasks</span><br><span class="line"><span class="keyword">import</span> joblib</span><br><span class="line"><span class="keyword">import</span> librosa</span><br><span class="line"><span class="keyword">from</span> moviepy.editor <span class="keyword">import</span> AudioFileClip</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 最大句子间隔</span></span><br><span class="line">max_sentence_interval = <span class="number">800</span></span><br><span class="line"><span class="comment"># 语气词合入极限间隔</span></span><br><span class="line">max_combine_interval = <span class="number">100</span></span><br><span class="line"><span class="comment"># 最小识别块，默认只识别不小于1000ms的块</span></span><br><span class="line">min_chunk_length = <span class="number">1000</span></span><br><span class="line"><span class="comment"># 余裕量,似乎funasr在词对齐上做得比较好，几乎不需要余裕了</span></span><br><span class="line">margin = <span class="number">0</span></span><br><span class="line"><span class="comment"># 说话人识别模型路径</span></span><br><span class="line">speaker_clf_path = <span class="string">&quot;说话人分类模型文件夹/clf.joblib&quot;</span></span><br><span class="line">id2speaker_path = <span class="string">&quot;说话人分类模型文件夹/id2speaker.json&quot;</span></span><br><span class="line"><span class="comment"># 嵌入模型的目标采样率</span></span><br><span class="line">target_sr = <span class="number">16000</span></span><br><span class="line"><span class="comment"># 输入路径</span></span><br><span class="line">audio_file = <span class="string">&quot;想切分的音频文件路径 xx/yy.wav&quot;</span></span><br><span class="line"><span class="comment"># 输出路径</span></span><br><span class="line">chunks_dir = <span class="string">&quot;输出chunks的文件夹路径&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">inference_pipeline = pipeline(</span><br><span class="line">    task=Tasks.auto_speech_recognition,</span><br><span class="line">    model=<span class="string">&#x27;damo/speech_paraformer-large-vad-punc_asr_nat-zh-cn-16k-common-vocab8404-pytorch&#x27;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">sv_pipline = pipeline(</span><br><span class="line">    task=<span class="string">&#x27;speaker-verification&#x27;</span>,</span><br><span class="line">    model=<span class="string">&#x27;damo/speech_eres2net_large_sv_zh-cn_3dspeaker_16k&#x27;</span>,</span><br><span class="line">    model_revision=<span class="string">&#x27;v1.0.0&#x27;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载说话人分类模型</span></span><br><span class="line">speaker_clf = joblib.load(speaker_clf_path)</span><br><span class="line">id2speaker = json.load(<span class="built_in">open</span>(id2speaker_path))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 转录音频</span></span><br><span class="line">rec_result = inference_pipeline(audio_in=audio_file)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将太远的词分割开，单独作为一个句子推理</span></span><br><span class="line">new_sentences = []</span><br><span class="line"><span class="keyword">for</span> sentence <span class="keyword">in</span> rec_result[<span class="string">&quot;sentences&quot;</span>]:</span><br><span class="line">    <span class="comment"># 准备左右对齐</span></span><br><span class="line">    ts_list = sentence[<span class="string">&quot;ts_list&quot;</span>]</span><br><span class="line">    word_seg_list = sentence[<span class="string">&quot;text_seg&quot;</span>].split(<span class="string">&quot; &quot;</span>)[:-<span class="number">1</span>]</span><br><span class="line">    text = sentence[<span class="string">&quot;text&quot;</span>]</span><br><span class="line">    start = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(ts_list)):</span><br><span class="line">        <span class="comment"># 检查前一个词的结束时间和后一个词的开始时间是否超出阈值</span></span><br><span class="line">        <span class="comment"># 如果超过阈值就手动把它们分隔开</span></span><br><span class="line">        <span class="keyword">if</span> ts_list[i][<span class="number">0</span>] - ts_list[i-<span class="number">1</span>][<span class="number">1</span>] &gt; max_sentence_interval:</span><br><span class="line">            <span class="comment"># 寻找第i个值并把start到i-1的字符加入到新文本中</span></span><br><span class="line">            <span class="comment"># 注意忽略掉新句子中的符号</span></span><br><span class="line">            <span class="comment"># 其实我感觉有点多余，因为这边的转录结果是不用的，但是还是这么做一下吧</span></span><br><span class="line">            rest_of_char = i - start</span><br><span class="line">            new_text = <span class="string">&quot;&quot;</span></span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(start, <span class="built_in">len</span>(text)):</span><br><span class="line">                <span class="keyword">if</span> rest_of_char &gt; <span class="number">0</span>:</span><br><span class="line">                    new_text += text[j]</span><br><span class="line"></span><br><span class="line">                    <span class="keyword">if</span> text[j] <span class="keyword">not</span> <span class="keyword">in</span> [<span class="string">&quot;，&quot;</span>, <span class="string">&quot;。&quot;</span>, <span class="string">&quot;！&quot;</span>]:</span><br><span class="line">                        rest_of_char -= <span class="number">1</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">            <span class="comment"># 合成新句子</span></span><br><span class="line">            new_sentence_data = &#123;</span><br><span class="line">                <span class="string">&quot;text&quot;</span>: new_text + <span class="string">&quot;。&quot;</span>, </span><br><span class="line">                <span class="string">&quot;start&quot;</span>: ts_list[start][<span class="number">0</span>], </span><br><span class="line">                <span class="string">&quot;end&quot;</span>: ts_list[i-<span class="number">1</span>][<span class="number">1</span>], </span><br><span class="line">                <span class="string">&quot;ts_list&quot;</span>: [ts_list[k] <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(start, i)]</span><br><span class="line">            &#125;</span><br><span class="line">            new_sentences.append(new_sentence_data)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 处理善后</span></span><br><span class="line">            <span class="comment"># print(j)</span></span><br><span class="line">            start = j</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 如果剩下的都正常就把关键信息提取并加进去</span></span><br><span class="line">    new_sentence_data = &#123;</span><br><span class="line">        <span class="string">&quot;text&quot;</span>: text[start: ], </span><br><span class="line">        <span class="string">&quot;start&quot;</span>: ts_list[start][<span class="number">0</span>], </span><br><span class="line">        <span class="string">&quot;end&quot;</span>: ts_list[-<span class="number">1</span>][<span class="number">1</span>], </span><br><span class="line">        <span class="string">&quot;ts_list&quot;</span>: [ts_list[k] <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(start, <span class="built_in">len</span>(ts_list))]</span><br><span class="line">    &#125;</span><br><span class="line">    new_sentences.append(new_sentence_data)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载audio文件并按目标频率采样为numpy对象</span></span><br><span class="line">audio_data, fs = librosa.load(audio_file, sr=target_sr)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取嵌入</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_embedding</span>(<span class="params">audio_data: np.ndarray</span>):</span><br><span class="line">    <span class="comment"># 把模型移到cuda上</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">next</span>(sv_pipline.model.parameters()).device.<span class="built_in">type</span> != <span class="string">&quot;cuda&quot;</span>:</span><br><span class="line">        sv_pipline.model.to(<span class="string">&quot;cuda&quot;</span>)</span><br><span class="line"></span><br><span class="line">    output = torch.from_numpy(audio_data).unsqueeze(<span class="number">0</span>)</span><br><span class="line">    output = output.to(<span class="string">&quot;cuda&quot;</span>)</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        embedding = sv_pipline.model(output)</span><br><span class="line">    <span class="keyword">return</span> embedding</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算每个片段的持续时间，超过1s的会送进去分类</span></span><br><span class="line"><span class="comment"># 短于1s的视为语气词，暂时不做分类（实际上分类的精度也很低）</span></span><br><span class="line"><span class="keyword">for</span> sentence <span class="keyword">in</span> new_sentences:</span><br><span class="line">    <span class="comment"># 获取说话人id</span></span><br><span class="line">    <span class="comment"># 先裁剪片段</span></span><br><span class="line">    sentence[<span class="string">&quot;duration&quot;</span>] = sentence[<span class="string">&quot;end&quot;</span>] - sentence[<span class="string">&quot;start&quot;</span>]</span><br><span class="line">    <span class="keyword">if</span> sentence[<span class="string">&quot;duration&quot;</span>] &lt; min_chunk_length:</span><br><span class="line">        <span class="comment"># 如果小于1s的片段就不做分类</span></span><br><span class="line">        sentence[<span class="string">&quot;speaker_id&quot;</span>] = -<span class="number">1</span></span><br><span class="line">        <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 大于1s的我们截取该段</span></span><br><span class="line">    start_sample = <span class="built_in">int</span>((sentence[<span class="string">&quot;start&quot;</span>] / <span class="number">1000</span>) * fs)</span><br><span class="line">    end_sample = <span class="built_in">int</span>((sentence[<span class="string">&quot;end&quot;</span>] / <span class="number">1000</span>) * fs)</span><br><span class="line">    target_audio_data = audio_data[start_sample: end_sample]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获取说话人id</span></span><br><span class="line">    embedding = get_embedding(target_audio_data).cpu().numpy()</span><br><span class="line">    result = speaker_clf.predict_proba(embedding)[<span class="number">0</span>]</span><br><span class="line">    speaker_id, proba = np.argmax(result), <span class="built_in">max</span>(result)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 记录说话人id、概率和说话人名字</span></span><br><span class="line">    sentence[<span class="string">&quot;speaker_id&quot;</span>], sentence[<span class="string">&quot;speaker_proba&quot;</span>] = speaker_id, proba</span><br><span class="line">    sentence[<span class="string">&quot;speaker&quot;</span>] = id2speaker[<span class="built_in">str</span>(speaker_id)]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">processed_sentences = []</span><br><span class="line"></span><br><span class="line">sentence_num_before_processing = <span class="built_in">len</span>(new_sentences)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(sentence_num_before_processing):</span><br><span class="line">    <span class="comment"># 按句子类型进行操作</span></span><br><span class="line">    <span class="keyword">if</span> new_sentences[i][<span class="string">&quot;speaker_id&quot;</span>] == -<span class="number">1</span>:</span><br><span class="line">        <span class="comment"># 语气词的处理</span></span><br><span class="line">        <span class="comment"># 增强阶段不考虑语气词处理</span></span><br><span class="line">        <span class="keyword">continue</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        processed_sentences.append(new_sentences[i])</span><br><span class="line">        </span><br><span class="line"><span class="comment"># 试着切一下看看效果</span></span><br><span class="line"><span class="comment"># 切分音频并保存</span></span><br><span class="line">basename = os.path.basename(audio_file)</span><br><span class="line">basename, ext_name = os.path.splitext(basename)</span><br><span class="line"><span class="comment"># 清除掉多余的后缀，如果需要的话...</span></span><br><span class="line">basename = basename.replace(<span class="string">&quot;_(Vocals)_UVR-MDX-NET-Voc_FT&quot;</span>, <span class="string">&quot;&quot;</span>)</span><br><span class="line"><span class="comment"># 保存音频的代码还是用AudioFileClip吧，用别的有点不习惯</span></span><br><span class="line">audio = AudioFileClip(audio_file)</span><br><span class="line"></span><br><span class="line">j = <span class="number">0</span></span><br><span class="line">all_ = <span class="built_in">len</span>(processed_sentences)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> sentence <span class="keyword">in</span> processed_sentences:</span><br><span class="line">    start = sentence[<span class="string">&quot;start&quot;</span>] / <span class="number">1000</span></span><br><span class="line">    end = sentence[<span class="string">&quot;end&quot;</span>] / <span class="number">1000</span></span><br><span class="line">    speaker = sentence[<span class="string">&quot;speaker&quot;</span>]</span><br><span class="line">    j += <span class="number">1</span></span><br><span class="line">    audio_chunk = audio.subclip(start, end)</span><br><span class="line">    <span class="comment"># 创建文件夹</span></span><br><span class="line">    os.makedirs(os.path.join(chunks_dir, speaker), exist_ok=<span class="literal">True</span>)</span><br><span class="line">    audio_chunk.write_audiofile(<span class="string">f&quot;<span class="subst">&#123;os.path.join(chunks_dir, speaker, basename)&#125;</span>_<span class="subst">&#123;j&#125;</span><span class="subst">&#123;ext_name&#125;</span>&quot;</span>)</span><br><span class="line">    </span><br><span class="line">audio.close()</span><br></pre></td></tr></table></figure>
<p>运行<code>enhance_clf.py</code>之后应该能得到一系列长于1s的音频，而且按基础说话人分类的结果将这些音频都归类到不同的说话人文件夹下面了。此时我们需要逐个检查这些文件，将不对应的音频文件（即分类器判断错误的）放入正确的说话人文件夹中。这里有一个需要注意的，如果你发现某个音频是有多个说话人或者是没处理干净的BGM的话，删除该音频避免它在后面混入训练数据中干扰模型。</p>
<p>最后，我们能够将所有音频检查完，得到一个干净的新数据集。重命名该文件夹，比如如果一开始放在<code>/root/data/processed/chunks</code>下的，在完成后可以重命名<code>chunks</code>文件夹为该集首字母的组合（这个随意，只是怕误删掉），比如切分的这一集是“大雄的黑洞”就重命名为<code>dxdhd_chunks</code>。</p>
<p>接下来要将这个新数据集加入到模型中训练一个更好的分类模型，在<code>feature_voice.py</code>中将新数据文件夹加入<code>extra_data_dirs</code>这个列表里即可：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># feature_voice.py</span></span><br><span class="line"><span class="comment"># 额外监督数据，如果生成的片段进行了校准，可以加入数据集中训练更好的标注模型</span></span><br><span class="line">extra_data_dirs = [<span class="string">&quot;/root/data/processed/dxdhd_chunks&quot;</span>]</span><br></pre></td></tr></table></figure>
<p>再次执行<code>feature_voice.py</code>可以训练一个新的更好的说话人分类模型并进行保存。</p>
<p>做完这章的步骤后，继续切另外的音频文件看看效果，如果效果不满意那就重复上述流程，随着标记数据量的增多，模型的分类错误率应该逐步下降。当我们觉得分类器的分类性能令人满意了之后，我们可以进入最后的一个步骤——组合ASR和说话人分类模型从音频中分离不同说话人。</p>
<h2
id="组合asr和说话人分类模型从音频中分离不同说话人">组合ASR和说话人分类模型从音频中分离不同说话人</h2>
<p>最后一步了，我们将上一个步骤训练好的说话人分类模型和ASR模型组合起来，在整个待处理音频文件夹上应用ASR切分+说话人分类。但也有一些和之前不同的地方：</p>
<ol type="1">
<li>将启发式地处理这些小于1s的音频块，因为这些音频一般是语气词，将其统称为语气片段。如果这个语气片段的左边或右边存在被标记为说话人的音频，而且间隔小于语气词合入极限间隔，那就会就近合入一个说话人中。</li>
<li>会将间隔小于预定义的最大句子间隔而且说话人相同的短句合为一个长句，这样切分的片段听起来更自然。</li>
</ol>
<p>下面是最后切分脚本的参考实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># splitter_by_asr.py</span></span><br><span class="line"><span class="keyword">from</span> modelscope.pipelines <span class="keyword">import</span> pipeline</span><br><span class="line"><span class="keyword">from</span> modelscope.utils.constant <span class="keyword">import</span> Tasks</span><br><span class="line"><span class="keyword">import</span> joblib</span><br><span class="line"><span class="keyword">import</span> librosa</span><br><span class="line"><span class="keyword">from</span> moviepy.editor <span class="keyword">import</span> AudioFileClip</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> glob</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 最大句子间隔</span></span><br><span class="line">max_sentence_interval = <span class="number">800</span></span><br><span class="line"><span class="comment"># 语气词合入极限间隔</span></span><br><span class="line">max_combine_interval = <span class="number">100</span></span><br><span class="line"><span class="comment"># 最小识别块，默认只识别不小于1000ms的块</span></span><br><span class="line">min_chunk_length = <span class="number">1000</span></span><br><span class="line"><span class="comment"># 余裕量,似乎funasr在词对齐上做得比较好，几乎不需要余裕了</span></span><br><span class="line">margin = <span class="number">0</span></span><br><span class="line"><span class="comment"># 说话人识别模型路径</span></span><br><span class="line">speaker_clf_path = <span class="string">&quot;说话人识别模型文件夹/clf.joblib&quot;</span></span><br><span class="line">id2speaker_path = <span class="string">&quot;说话人识别模型文件夹/id2speaker.json&quot;</span></span><br><span class="line"><span class="comment"># 嵌入模型的目标采样率</span></span><br><span class="line">target_sr = <span class="number">16000</span></span><br><span class="line"><span class="comment"># 存放需要切分和说话人识别的音频文件夹路径</span></span><br><span class="line">audio_path = <span class="string">&quot;需要切分和说话人识别的音频文件夹路径&quot;</span></span><br><span class="line"><span class="comment"># 输出路径</span></span><br><span class="line">chunks_dir = <span class="string">&quot;输出的路径 xxx/raw类似的&quot;</span></span><br><span class="line"><span class="comment"># token批量大小</span></span><br><span class="line">batch_size_token = <span class="number">4000</span></span><br><span class="line"><span class="comment"># 起始位置，如果爆显存了调整该值跳过已经转录的样本</span></span><br><span class="line">start_pos = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">inference_pipeline = pipeline(</span><br><span class="line">    task=Tasks.auto_speech_recognition,</span><br><span class="line">    model=<span class="string">&#x27;damo/speech_paraformer-large-vad-punc_asr_nat-zh-cn-16k-common-vocab8404-pytorch&#x27;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">sv_pipline = pipeline(</span><br><span class="line">    task=<span class="string">&#x27;speaker-verification&#x27;</span>,</span><br><span class="line">    model=<span class="string">&#x27;damo/speech_eres2net_large_sv_zh-cn_3dspeaker_16k&#x27;</span>,</span><br><span class="line">    model_revision=<span class="string">&#x27;v1.0.0&#x27;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载说话人分类模型</span></span><br><span class="line">speaker_clf = joblib.load(speaker_clf_path)</span><br><span class="line">id2speaker = json.load(<span class="built_in">open</span>(id2speaker_path))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取嵌入</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_embedding</span>(<span class="params">audio_data: np.ndarray</span>):</span><br><span class="line">    <span class="comment"># 把模型移到cuda上</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">next</span>(sv_pipline.model.parameters()).device.<span class="built_in">type</span> != <span class="string">&quot;cuda&quot;</span>:</span><br><span class="line">        sv_pipline.model.to(<span class="string">&quot;cuda&quot;</span>)</span><br><span class="line"></span><br><span class="line">    output = torch.from_numpy(audio_data).unsqueeze(<span class="number">0</span>)</span><br><span class="line">    output = output.to(<span class="string">&quot;cuda&quot;</span>)</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        embedding = sv_pipline.model(output)</span><br><span class="line">    <span class="keyword">return</span> embedding</span><br><span class="line"></span><br><span class="line">audio_files = glob.glob(os.path.join(audio_path, <span class="string">&quot;*.wav&quot;</span>))</span><br><span class="line"><span class="comment"># 跳过已经转录的音频</span></span><br><span class="line">audio_files = audio_files[start_pos: ]</span><br><span class="line">all_num = <span class="built_in">len</span>(audio_files)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 逐个处理每个音频</span></span><br><span class="line"><span class="keyword">for</span> a_i, audio_file <span class="keyword">in</span> <span class="built_in">enumerate</span>(audio_files):</span><br><span class="line">    <span class="comment"># 转录音频</span></span><br><span class="line">    rec_result = inference_pipeline(audio_in=audio_file, batch_size_token=batch_size_token)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将太远的词分割开，单独作为一个句子推理</span></span><br><span class="line">    new_sentences = []</span><br><span class="line">    <span class="keyword">for</span> sentence <span class="keyword">in</span> rec_result[<span class="string">&quot;sentences&quot;</span>]:</span><br><span class="line">        <span class="comment"># 准备左右对齐</span></span><br><span class="line">        ts_list = sentence[<span class="string">&quot;ts_list&quot;</span>]</span><br><span class="line">        word_seg_list = sentence[<span class="string">&quot;text_seg&quot;</span>].split(<span class="string">&quot; &quot;</span>)[:-<span class="number">1</span>]</span><br><span class="line">        text = sentence[<span class="string">&quot;text&quot;</span>]</span><br><span class="line">        start = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(ts_list)):</span><br><span class="line">            <span class="comment"># 检查前一个词的结束时间和后一个词的开始时间是否超出阈值</span></span><br><span class="line">            <span class="comment"># 如果超过阈值就手动把它们分隔开</span></span><br><span class="line">            <span class="keyword">if</span> ts_list[i][<span class="number">0</span>] - ts_list[i-<span class="number">1</span>][<span class="number">1</span>] &gt; max_sentence_interval:</span><br><span class="line">                <span class="comment"># 寻找第i个值并把start到i-1的字符加入到新文本中</span></span><br><span class="line">                <span class="comment"># 注意忽略掉新句子中的符号</span></span><br><span class="line">                <span class="comment"># 其实我感觉有点多余，因为这边的转录结果是不用的，但是还是这么做一下吧</span></span><br><span class="line">                rest_of_char = i - start</span><br><span class="line">                new_text = <span class="string">&quot;&quot;</span></span><br><span class="line">                <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(start, <span class="built_in">len</span>(text)):</span><br><span class="line">                    <span class="keyword">if</span> rest_of_char &gt; <span class="number">0</span>:</span><br><span class="line">                        new_text += text[j]</span><br><span class="line"></span><br><span class="line">                        <span class="keyword">if</span> text[j] <span class="keyword">not</span> <span class="keyword">in</span> [<span class="string">&quot;，&quot;</span>, <span class="string">&quot;。&quot;</span>, <span class="string">&quot;！&quot;</span>]:</span><br><span class="line">                            rest_of_char -= <span class="number">1</span></span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        <span class="keyword">break</span></span><br><span class="line">                <span class="comment"># 合成新句子</span></span><br><span class="line">                new_sentence_data = &#123;</span><br><span class="line">                    <span class="string">&quot;text&quot;</span>: new_text + <span class="string">&quot;。&quot;</span>, </span><br><span class="line">                    <span class="string">&quot;start&quot;</span>: ts_list[start][<span class="number">0</span>], </span><br><span class="line">                    <span class="string">&quot;end&quot;</span>: ts_list[i-<span class="number">1</span>][<span class="number">1</span>], </span><br><span class="line">                    <span class="string">&quot;ts_list&quot;</span>: [ts_list[k] <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(start, i)]</span><br><span class="line">                &#125;</span><br><span class="line">                new_sentences.append(new_sentence_data)</span><br><span class="line">                </span><br><span class="line">                <span class="comment"># 处理善后</span></span><br><span class="line">                <span class="comment"># print(j)</span></span><br><span class="line">                start = j</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 如果剩下的都正常就把关键信息提取并加进去</span></span><br><span class="line">        new_sentence_data = &#123;</span><br><span class="line">            <span class="string">&quot;text&quot;</span>: text[start: ], </span><br><span class="line">            <span class="string">&quot;start&quot;</span>: ts_list[start][<span class="number">0</span>], </span><br><span class="line">            <span class="string">&quot;end&quot;</span>: ts_list[-<span class="number">1</span>][<span class="number">1</span>], </span><br><span class="line">            <span class="string">&quot;ts_list&quot;</span>: [ts_list[k] <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(start, <span class="built_in">len</span>(ts_list))]</span><br><span class="line">        &#125;</span><br><span class="line">        new_sentences.append(new_sentence_data)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 加载audio文件并按目标频率采样为numpy对象</span></span><br><span class="line">    audio_data, fs = librosa.load(audio_file, sr=target_sr)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算每个片段的持续时间，超过1s的会送进去分类</span></span><br><span class="line">    <span class="comment"># 短于1s的视为语气词，暂时不做分类（实际上分类的精度也很低）</span></span><br><span class="line">    <span class="keyword">for</span> sentence <span class="keyword">in</span> new_sentences:</span><br><span class="line">        <span class="comment"># 获取说话人id</span></span><br><span class="line">        <span class="comment"># 先裁剪片段</span></span><br><span class="line">        sentence[<span class="string">&quot;duration&quot;</span>] = sentence[<span class="string">&quot;end&quot;</span>] - sentence[<span class="string">&quot;start&quot;</span>]</span><br><span class="line">        <span class="keyword">if</span> sentence[<span class="string">&quot;duration&quot;</span>] &lt; min_chunk_length:</span><br><span class="line">            <span class="comment"># 如果小于1s的片段就先不做分类</span></span><br><span class="line">            sentence[<span class="string">&quot;speaker_id&quot;</span>] = -<span class="number">1</span></span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 大于1s的我们截取该段</span></span><br><span class="line">        start_sample = <span class="built_in">int</span>((sentence[<span class="string">&quot;start&quot;</span>] / <span class="number">1000</span>) * fs)</span><br><span class="line">        end_sample = <span class="built_in">int</span>((sentence[<span class="string">&quot;end&quot;</span>] / <span class="number">1000</span>) * fs)</span><br><span class="line">        target_audio_data = audio_data[start_sample: end_sample]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 获取说话人id</span></span><br><span class="line">        embedding = get_embedding(target_audio_data).cpu().numpy()</span><br><span class="line">        result = speaker_clf.predict_proba(embedding)[<span class="number">0</span>]</span><br><span class="line">        speaker_id, proba = np.argmax(result), <span class="built_in">max</span>(result)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 记录说话人id、概率和说话人名字</span></span><br><span class="line">        sentence[<span class="string">&quot;speaker_id&quot;</span>], sentence[<span class="string">&quot;speaker_proba&quot;</span>] = speaker_id, proba</span><br><span class="line">        sentence[<span class="string">&quot;speaker&quot;</span>] = id2speaker[<span class="built_in">str</span>(speaker_id)]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 合并相同说话人片段和语气片段</span></span><br><span class="line">    <span class="comment"># 请注意这些语气片段应该有一个合成时间，大于该时间的不应该合入</span></span><br><span class="line">    processed_sentences = []</span><br><span class="line">    <span class="comment"># 由后到前合并这些句子，注意对说话人相同的句子按句子间隔合入，对语气词按极限语气词合入间隔合入</span></span><br><span class="line">    sentence_num_before_processing = <span class="built_in">len</span>(new_sentences)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(sentence_num_before_processing-<span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>):</span><br><span class="line">        <span class="comment"># 按句子类型进行操作</span></span><br><span class="line">        <span class="keyword">if</span> new_sentences[i][<span class="string">&quot;speaker_id&quot;</span>] == -<span class="number">1</span>:</span><br><span class="line">            <span class="comment"># 语气词的处理</span></span><br><span class="line">            <span class="comment"># 先考虑特殊情况</span></span><br><span class="line">            <span class="keyword">if</span> i == sentence_num_before_processing - <span class="number">1</span>:</span><br><span class="line">                <span class="keyword">if</span> new_sentences[i][<span class="string">&quot;start&quot;</span>] - new_sentences[i-<span class="number">1</span>][<span class="string">&quot;end&quot;</span>] &lt; max_combine_interval <span class="keyword">and</span> \</span><br><span class="line">                new_sentences[i-<span class="number">1</span>][<span class="string">&quot;speaker_id&quot;</span>] != -<span class="number">1</span>:</span><br><span class="line">                    <span class="comment"># 满足条件的尾句合入前一个实句中</span></span><br><span class="line">                    new_sentences[i-<span class="number">1</span>][<span class="string">&quot;end&quot;</span>] = new_sentences[i][<span class="string">&quot;end&quot;</span>]</span><br><span class="line">                    new_sentences[i-<span class="number">1</span>][<span class="string">&quot;text&quot;</span>] = new_sentences[i-<span class="number">1</span>][<span class="string">&quot;text&quot;</span>] + new_sentences[i][<span class="string">&quot;text&quot;</span>]</span><br><span class="line">            <span class="keyword">elif</span> i == <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">if</span> <span class="built_in">len</span>(processed_sentences) &gt; <span class="number">0</span> <span class="keyword">and</span> <span class="built_in">len</span>(new_sentences) &gt; <span class="number">1</span> <span class="keyword">and</span> \</span><br><span class="line">                processed_sentences[<span class="number">0</span>][<span class="string">&quot;start&quot;</span>] - new_sentences[i][<span class="string">&quot;end&quot;</span>] &lt; max_combine_interval:</span><br><span class="line">                    <span class="comment"># 满足条件的首句合入后一个实句中，这一般不太可能发生</span></span><br><span class="line">                    <span class="comment"># 因为第一句一般是报幕，就算低于1s也不可能只有300ms间隔</span></span><br><span class="line">                    processed_sentences[<span class="number">0</span>][<span class="string">&quot;start&quot;</span>] = new_sentences[i][<span class="string">&quot;start&quot;</span>]</span><br><span class="line">                    processed_sentences[<span class="number">0</span>][<span class="string">&quot;text&quot;</span>] = new_sentences[i][<span class="string">&quot;text&quot;</span>] + processed_sentences[<span class="number">0</span>][<span class="string">&quot;text&quot;</span>]</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="comment"># 中间情况，一般是左右看然后选最优的加入</span></span><br><span class="line">                min_duration = max_sentence_interval + <span class="number">1</span></span><br><span class="line">                min_pos = <span class="literal">None</span></span><br><span class="line">                <span class="comment"># 先往左看</span></span><br><span class="line">                <span class="keyword">if</span> new_sentences[i][<span class="string">&quot;start&quot;</span>] - new_sentences[i-<span class="number">1</span>][<span class="string">&quot;end&quot;</span>] &lt; max_combine_interval <span class="keyword">and</span> \</span><br><span class="line">                new_sentences[i-<span class="number">1</span>][<span class="string">&quot;speaker_id&quot;</span>] != -<span class="number">1</span>:</span><br><span class="line">                    <span class="comment"># 更新最小间隔</span></span><br><span class="line">                    min_duration = new_sentences[i][<span class="string">&quot;start&quot;</span>] - new_sentences[i-<span class="number">1</span>][<span class="string">&quot;end&quot;</span>]</span><br><span class="line">                    min_pos = <span class="string">&quot;left&quot;</span></span><br><span class="line">                <span class="comment"># 再往右看</span></span><br><span class="line">                <span class="keyword">if</span> <span class="built_in">len</span>(processed_sentences) &gt; <span class="number">0</span> <span class="keyword">and</span> <span class="built_in">len</span>(new_sentences) &gt; <span class="number">1</span> <span class="keyword">and</span> \</span><br><span class="line">                processed_sentences[<span class="number">0</span>][<span class="string">&quot;start&quot;</span>] - new_sentences[i][<span class="string">&quot;end&quot;</span>] &lt; max_combine_interval:</span><br><span class="line">                    <span class="keyword">if</span> processed_sentences[<span class="number">0</span>][<span class="string">&quot;start&quot;</span>] - new_sentences[i][<span class="string">&quot;end&quot;</span>] &lt; min_duration:</span><br><span class="line">                        min_duration = processed_sentences[<span class="number">0</span>][<span class="string">&quot;start&quot;</span>] - new_sentences[i][<span class="string">&quot;end&quot;</span>]</span><br><span class="line">                        min_pos = <span class="string">&quot;right&quot;</span></span><br><span class="line">                </span><br><span class="line">                <span class="comment"># 合入句子中</span></span><br><span class="line">                <span class="keyword">if</span> min_pos <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                    <span class="keyword">if</span> min_pos == <span class="string">&quot;left&quot;</span>:</span><br><span class="line">                        new_sentences[i-<span class="number">1</span>][<span class="string">&quot;end&quot;</span>] = new_sentences[i][<span class="string">&quot;end&quot;</span>]</span><br><span class="line">                        new_sentences[i-<span class="number">1</span>][<span class="string">&quot;text&quot;</span>] = new_sentences[i-<span class="number">1</span>][<span class="string">&quot;text&quot;</span>] + new_sentences[i][<span class="string">&quot;text&quot;</span>]</span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        processed_sentences[<span class="number">0</span>][<span class="string">&quot;start&quot;</span>] = new_sentences[i][<span class="string">&quot;start&quot;</span>]</span><br><span class="line">                        processed_sentences[<span class="number">0</span>][<span class="string">&quot;text&quot;</span>] = new_sentences[i][<span class="string">&quot;text&quot;</span>] + processed_sentences[<span class="number">0</span>][<span class="string">&quot;text&quot;</span>]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 在这个分支的是实句，只考虑向后进行合并</span></span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(processed_sentences) == <span class="number">0</span>:</span><br><span class="line">                <span class="comment"># 没有句子的话直接放进去</span></span><br><span class="line">                processed_sentences.append(new_sentences[i])</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="comment"># 先看看能不能合入</span></span><br><span class="line">                <span class="keyword">if</span> processed_sentences[<span class="number">0</span>][<span class="string">&quot;speaker_id&quot;</span>] == new_sentences[i][<span class="string">&quot;speaker_id&quot;</span>] <span class="keyword">and</span> \</span><br><span class="line">                processed_sentences[<span class="number">0</span>][<span class="string">&quot;start&quot;</span>] - new_sentences[i][<span class="string">&quot;end&quot;</span>] &lt; max_sentence_interval:</span><br><span class="line">                    processed_sentences[<span class="number">0</span>][<span class="string">&quot;text&quot;</span>] = new_sentences[i][<span class="string">&quot;text&quot;</span>] + processed_sentences[<span class="number">0</span>][<span class="string">&quot;text&quot;</span>]</span><br><span class="line">                    processed_sentences[<span class="number">0</span>][<span class="string">&quot;start&quot;</span>] = new_sentences[i][<span class="string">&quot;start&quot;</span>]</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    <span class="comment"># 不能合入就作为独立句子插入列表首</span></span><br><span class="line">                    processed_sentences.insert(<span class="number">0</span>, new_sentences[i])</span><br><span class="line">                    </span><br><span class="line">    <span class="comment"># 切分音频并保存</span></span><br><span class="line">    basename = os.path.basename(audio_file)</span><br><span class="line">    basename, ext_name = os.path.splitext(basename)</span><br><span class="line">    <span class="comment"># 清除掉多余的后缀，如果需要的话...</span></span><br><span class="line">    basename = basename.replace(<span class="string">&quot;_(Vocals)_UVR-MDX-NET-Voc_FT&quot;</span>, <span class="string">&quot;&quot;</span>)</span><br><span class="line">    <span class="comment"># 保存音频的代码还是用AudioFileClip吧，用别的有点不习惯</span></span><br><span class="line">    audio = AudioFileClip(audio_file)</span><br><span class="line"></span><br><span class="line">    j = <span class="number">0</span></span><br><span class="line">    all_ = <span class="built_in">len</span>(processed_sentences)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> sentence <span class="keyword">in</span> processed_sentences:</span><br><span class="line">        start = sentence[<span class="string">&quot;start&quot;</span>] / <span class="number">1000</span></span><br><span class="line">        end = sentence[<span class="string">&quot;end&quot;</span>] / <span class="number">1000</span></span><br><span class="line">        speaker = sentence[<span class="string">&quot;speaker&quot;</span>]</span><br><span class="line">        j += <span class="number">1</span></span><br><span class="line">        audio_chunk = audio.subclip(start, end)</span><br><span class="line">        <span class="comment"># 创建文件夹</span></span><br><span class="line">        os.makedirs(os.path.join(chunks_dir, speaker), exist_ok=<span class="literal">True</span>)</span><br><span class="line">        audio_chunk.write_audiofile(<span class="string">f&quot;<span class="subst">&#123;os.path.join(chunks_dir, speaker, basename)&#125;</span>_<span class="subst">&#123;j&#125;</span><span class="subst">&#123;ext_name&#125;</span>&quot;</span>)</span><br><span class="line">        </span><br><span class="line">    audio.close()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;已处理: <span class="subst">&#123;a_i + <span class="number">1</span>&#125;</span> / <span class="subst">&#123;all_num&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;处理完成!&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>等待处理完成后，应该在<code>chunks</code>变量对应的目录下看到最终的结果了，到这里就已经把音频成功切分并按说话人分好类了，完结撒花。</p>
<h1 id="总结与思考">总结与思考</h1>
<p>因为基于聚类的说话人分类比较吃embedding的余弦相似度判别效果，而主要在英文数据集上训练的<code>pyannote</code>对中文音频的判别效果不佳，而且要收集数据微调一个这样的纯音频说话人识别模型难度比较大，所以放弃了基于纯音频方案的分割。</p>
<p>作为替代，使用了ASR转录出来的文本信息中的语义停顿符号作为分割依据，这在一定程度上缓解了多说话人音频分割不准确的问题。然而基于语义信息的切分依然受到转录文本质量、标点符号生成质量和字级时间戳模型对齐精度的影响，尤其是时间戳对齐不准确的问题会导致音频提前开始或提早结束，这影响了切分后音频的质量。因此，这个方法目前来说的累计误差还是比较大的，还存在性能提升空间。并且它并不能处理多说话人说话重叠的问题，这似乎是这个方案的硬伤。</p>
<p>分割好了音频片段后，使用渐进式训练的方式通过人类反馈提升说话人分类模型的性能。在人类的监督下，有监督的说话人分离模型的表现应该远远好于基于聚类算法的说话人分类效果。同时，因为是渐进式地提升模型分类效果，所以理论上越到渐进式训练的后期需要人类调整的错误样本就越少，这有助于减少人类标注的时间成本。</p>
<p>这个流程仍然有一些值得思考的地方：</p>
<ol type="1">
<li>在最开始训练说话人分类基础模型时，纯粹依赖于手工挑选样本，这样非常的枯燥而且消耗人力。在这个步骤里面如果先聚类，通过聚类算法把相似的音频合在一堆然后再让人来挑选会不会能节省工作量。</li>
<li>在embedding后面接LDA模型（线性判别模型）完成说话人分类是因为我觉得它优化的原理来自“最大化类间距离，最小化类内距离”，这样的效果就好像是一个引入人类监督的“聚类算法”。通过LDA找到某个线性子空间，然后将样本投影到这个空间后就能发现不同说话人的音频数据点形成不同的团簇。这样降维的做法比较符合直觉，假设提取的音频embedding里包含了说话人详细的音色信息，但如果我们只需要分类男声和女声，那么其实很多音色信息都是冗余的而且可能带来误判，如果通过LDA裁剪这些高维的信息丰富的embedding到我们任务相关的低维空间，这样应该能在利用预训练的embedding和收集的任务相关的训练样本中得到一个比较好的平衡。但随着可用训练样本的增多，会不会使用更复杂的分类器，如随机森林、XGBoost能得到更稳定更好的效果呢？</li>
</ol>
<p>以上就是全部内容了，非常感谢这些音频领域的研究者们开放了这么多有价值的预训练模型！</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="https://lisheng2001.github.io">LiSheng</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://lisheng2001.github.io/2023/10/07/%E5%88%A9%E7%94%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B0%86%E4%B8%8D%E5%90%8C%E8%AF%B4%E8%AF%9D%E4%BA%BA%E4%BB%8E%E9%9F%B3%E9%A2%91%E4%B8%AD%E5%88%86%E7%A6%BB/">https://lisheng2001.github.io/2023/10/07/利用深度学习将不同说话人从音频中分离/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://lisheng2001.github.io" target="_blank">一世逍遥的博客</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E5%A4%9A%E6%A8%A1%E6%80%81/">多模态</a><a class="post-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><a class="post-meta__tags" href="/tags/%E8%AF%AD%E9%9F%B3%E8%AF%86%E5%88%AB/">语音识别</a><a class="post-meta__tags" href="/tags/%E8%AF%B4%E8%AF%9D%E4%BA%BA%E5%88%86%E7%B1%BB/">说话人分类</a></div><div class="post_share"><div class="social-share" data-image="/images/avatar.webp" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2023/10/10/%E5%88%B7%E9%A2%98%E5%88%86%E4%BA%AB%EF%BC%9A%E6%88%91%E5%86%99%E7%89%9B%E5%AE%A2TOP101%E9%81%93%E7%BC%96%E7%A8%8B%E9%A2%98/" title="刷题分享：我写牛客TOP101道编程题"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">刷题分享：我写牛客TOP101道编程题</div></div></a></div><div class="next-post pull-right"><a href="/2023/09/09/%E8%A1%A8%E8%BE%BE%E5%BC%8F%E8%AE%A1%E7%AE%97%E4%B8%93%E9%A2%98/" title="表达式计算专题"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">表达式计算专题</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2024/06/16/VQAScore-%E6%96%87%E7%94%9F%E5%9B%BE%E8%AF%84%E4%BB%B7%E6%96%B0%E6%8C%87%E6%A0%87/" title="VQAScore: 文生图评价新指标"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-06-16</div><div class="title">VQAScore: 文生图评价新指标</div></div></a></div><div><a href="/2023/04/13/%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB-%E5%9F%BA%E4%BA%8E%E6%8F%90%E7%A4%BA%E7%9A%84%E5%A4%9A%E6%A8%A1%E6%80%81%E4%BB%87%E6%81%A8%E8%BF%B7%E5%9B%A0%E5%88%86%E7%B1%BB/" title="论文分享-基于提示的多模态仇恨迷因分类"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-04-13</div><div class="title">论文分享-基于提示的多模态仇恨迷因分类</div></div></a></div><div><a href="/2023/08/26/%E9%87%8D%E6%96%B0%E8%AE%A4%E8%AF%86CLIP%E6%96%87%E6%9C%AC%E4%BE%A7%E7%BC%96%E7%A0%81%E5%99%A8/" title="重新认识CLIP文本侧编码器"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-08-26</div><div class="title">重新认识CLIP文本侧编码器</div></div></a></div><div><a href="/2025/01/06/huggingface%E4%B8%ADDatasets%E6%A8%A1%E5%9D%97%E7%9A%84%E7%AC%94%E8%AE%B0/" title="huggingface中Datasets模块的笔记"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-01-06</div><div class="title">huggingface中Datasets模块的笔记</div></div></a></div></div></div><hr class="custom-hr"/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="utterances-wrap"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/images/avatar.webp" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">LiSheng</div><div class="author-info__description">也无风雨也无晴</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">51</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">71</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">12</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/LiSheng2001"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%96%B9%E6%B3%95%E6%A6%82%E8%BF%B0"><span class="toc-number">1.</span> <span class="toc-text">方法概述</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%85%B7%E4%BD%93%E6%B5%81%E7%A8%8B%E5%8F%8A%E4%BE%9B%E5%8F%82%E8%80%83%E7%9A%84%E5%AE%9E%E7%8E%B0%E4%BB%A3%E7%A0%81"><span class="toc-number">2.</span> <span class="toc-text">具体流程及供参考的实现代码</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%8E%E8%A7%86%E9%A2%91%E4%B8%AD%E5%88%86%E7%A6%BB%E9%9F%B3%E9%A2%91%E5%8F%AF%E9%80%89"><span class="toc-number">2.1.</span> <span class="toc-text">从视频中分离音频（可选）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B0%86%E4%BA%BA%E5%A3%B0%E4%B8%8E%E8%83%8C%E6%99%AF%E9%9F%B3%E4%B9%90%E5%88%86%E7%A6%BB%E5%8F%AF%E9%80%89"><span class="toc-number">2.2.</span> <span class="toc-text">将人声与背景音乐分离（可选）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#asr%E9%A2%84%E5%88%87%E5%88%86"><span class="toc-number">2.3.</span> <span class="toc-text">ASR预切分</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AF%B4%E8%AF%9D%E4%BA%BA%E5%88%86%E7%B1%BB%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83"><span class="toc-number">2.4.</span> <span class="toc-text">说话人分类基础模型训练</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B8%90%E8%BF%9B%E5%9C%B0%E6%8F%90%E9%AB%98%E8%AF%B4%E8%AF%9D%E4%BA%BA%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%80%A7%E8%83%BD%E5%BE%AA%E7%8E%AF%E5%A4%9A%E6%AC%A1"><span class="toc-number">2.5.</span> <span class="toc-text">渐进地提高说话人分类模型的性能（循环多次）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BB%84%E5%90%88asr%E5%92%8C%E8%AF%B4%E8%AF%9D%E4%BA%BA%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E4%BB%8E%E9%9F%B3%E9%A2%91%E4%B8%AD%E5%88%86%E7%A6%BB%E4%B8%8D%E5%90%8C%E8%AF%B4%E8%AF%9D%E4%BA%BA"><span class="toc-number">2.6.</span> <span class="toc-text">组合ASR和说话人分类模型从音频中分离不同说话人</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%80%BB%E7%BB%93%E4%B8%8E%E6%80%9D%E8%80%83"><span class="toc-number">3.</span> <span class="toc-text">总结与思考</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/03/28/%E4%BB%8E3%E7%AF%87%E8%AE%BA%E6%96%87%E5%9B%9E%E9%A1%BE2024%E5%B9%B4%E5%9F%BA%E7%A1%80%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%BF%9B%E5%B1%95/" title="从3篇论文回顾2024年基础大语言模型的进展">从3篇论文回顾2024年基础大语言模型的进展</a><time datetime="2025-03-28T13:03:49.000Z" title="发表于 2025-03-28 21:03:49">2025-03-28</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/01/27/%E8%AF%84%E4%BC%B0LLM%E4%B9%8B%E5%9C%A824%E5%B9%B4%E7%9A%84%E7%AE%A1%E7%90%86%E7%B1%BB%E8%81%94%E8%80%83%E9%80%BB%E8%BE%91%E9%80%89%E6%8B%A9%E9%A2%98%E8%AF%84%E4%BC%B0/" title="评估LLM之在24年的管理类联考逻辑选择题评估">评估LLM之在24年的管理类联考逻辑选择题评估</a><time datetime="2025-01-27T08:33:07.000Z" title="发表于 2025-01-27 16:33:07">2025-01-27</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/01/23/%E5%9C%A8hexo%E5%8D%9A%E5%AE%A2%E4%B8%AD%E6%8F%92%E5%85%A5echarts%E5%9B%BE%E8%A1%A8/" title="在hexo博客中插入echarts图表">在hexo博客中插入echarts图表</a><time datetime="2025-01-23T03:59:36.000Z" title="发表于 2025-01-23 11:59:36">2025-01-23</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/01/06/huggingface%E4%B8%ADDatasets%E6%A8%A1%E5%9D%97%E7%9A%84%E7%AC%94%E8%AE%B0/" title="huggingface中Datasets模块的笔记">huggingface中Datasets模块的笔记</a><time datetime="2025-01-06T12:43:34.000Z" title="发表于 2025-01-06 20:43:34">2025-01-06</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/01/04/LLM%E8%AF%84%E4%BC%B0/" title="LLM评估">LLM评估</a><time datetime="2025-01-04T12:45:23.000Z" title="发表于 2025-01-04 20:45:23">2025-01-04</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2025 By LiSheng</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><script>function runPanguForPostContent() {
  const postContent = document.querySelector('.post-content');

  if (postContent) {
    pangu.spacingNode(postContent); // 只处理文章内容区域
  }
}

function panguFn() {
  if (typeof pangu === 'object') {
    runPanguForPostContent();
  } else {
    getScript('https://cdn.jsdelivr.net/npm/pangu/dist/browser/pangu.min.js')
      .then(() => {
        runPanguForPostContent();
      });
  }
}

function panguInit() {
  if (false) {
    GLOBAL_CONFIG_SITE.isPost && panguFn();
  } else {
    panguFn();
  }
}

document.addEventListener('DOMContentLoaded', panguInit);</script><div class="js-pjax"><script>(() => {
  const $mermaid = document.querySelectorAll('#article-container .mermaid-wrap')
  if ($mermaid.length === 0) return
  const runMermaid = () => {
    window.loadMermaid = true
    const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark' : 'default'

    Array.from($mermaid).forEach((item, index) => {
      const mermaidSrc = item.firstElementChild
      const mermaidThemeConfig = '%%{init:{ \'theme\':\'' + theme + '\'}}%%\n'
      const mermaidID = 'mermaid-' + index
      const mermaidDefinition = mermaidThemeConfig + mermaidSrc.textContent

      const renderFn = mermaid.render(mermaidID, mermaidDefinition)

      const renderV10 = () => {
        renderFn.then(({svg}) => {
          mermaidSrc.insertAdjacentHTML('afterend', svg)
        })
      }

      const renderV9 = svg => {
        mermaidSrc.insertAdjacentHTML('afterend', svg)
      }

      typeof renderFn === 'string' ? renderV9(renderFn) : renderV10()
    })
  }

  const loadMermaid = () => {
    window.loadMermaid ? runMermaid() : getScript('https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js').then(runMermaid)
  }

  btf.addModeChange('mermaid', runMermaid)

  window.pjax ? loadMermaid() : document.addEventListener('DOMContentLoaded', loadMermaid)
})()</script><script>function loadUtterances () {
  let ele = document.createElement('script')
  ele.setAttribute('id', 'utterances_comment')
  ele.setAttribute('src', 'https://utteranc.es/client.js')
  ele.setAttribute('repo', 'LiSheng2001/LiSheng2001.github.io')
  ele.setAttribute('issue-term', 'pathname')
  let nowTheme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'photon-dark' : 'github-light'
  ele.setAttribute('theme', nowTheme)
  ele.setAttribute('crossorigin', 'anonymous')
  ele.setAttribute('async', 'true')
  document.getElementById('utterances-wrap').insertAdjacentElement('afterbegin',ele)
}

function utterancesTheme (theme) {
  const iframe = document.querySelector('.utterances-frame')
  if (iframe) {
    const theme = theme === 'dark' ? 'photon-dark' : 'github-light'
    const message = {
      type: 'set-theme',
      theme: theme
    };
    iframe.contentWindow.postMessage(message, 'https://utteranc.es');
  }
}

btf.addModeChange('utterances', utterancesTheme)

if ('Utterances' === 'Utterances' || !true) {
  if (true) btf.loadComment(document.getElementById('utterances-wrap'), loadUtterances)
  else loadUtterances()
} else {
  function loadOtherComment () {
    loadUtterances()
  }
}</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>